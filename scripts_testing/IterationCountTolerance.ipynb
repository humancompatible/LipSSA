{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Number of sample paths\n",
    "This notebook illustrates average number of sample paths for relative and absolute tolerance approximation."
   ],
   "id": "478a409ccee4dd8a"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-20T13:43:18.730923Z",
     "start_time": "2025-05-20T13:43:16.963964Z"
    }
   },
   "source": [
    "import sys\n",
    "sys.path.append('..')  \n",
    "\n",
    "from relu_nets import ReLUNet  \n",
    "from hyperbox import Hyperbox  \n",
    "from lipMIP import LipMIP\n",
    "from helper_functions_demo import *\n",
    "from other_methods import StochasticApproximation, StochasticApproximationEqDiv, StochasticApproximationUCBDynamic"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T13:43:18.734544Z",
     "start_time": "2025-05-20T13:43:18.731940Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_trained_network(layer_sizes, rad, dim):\n",
    "    test_network = ReLUNet(layer_sizes)\n",
    "    num_points = 1000\n",
    "    X, y = get_random_dataset(num_points=num_points, radius=rad, num_lakes=3, dim=dim)\n",
    "    train_X = X[:int(num_points * 0.8)]\n",
    "    train_y = y[:int(num_points * 0.8)].unsqueeze(1)\n",
    "    # test_X = X[int(num_points * 0.8):]\n",
    "    # test_y = y[int(num_points * 0.8):].unsqueeze(1)\n",
    "\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(test_network.parameters(), lr=5e-4)\n",
    "\n",
    "    epochs = 500\n",
    "    for epoch in range(epochs):\n",
    "        test_network.train()\n",
    "        pred = test_network(train_X)\n",
    "        loss = loss_fn(pred, train_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # if epoch % 5 == 0:\n",
    "        #     test_network.eval()\n",
    "        #     pred = test_network(test_X)\n",
    "        #     loss = loss_fn(pred, test_y)\n",
    "        #     print(f\"Epoch {epoch}: Loss: {loss.item():.4f}\")\n",
    "        \n",
    "    return test_network"
   ],
   "id": "cd1d290874e74485",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T13:43:18.739148Z",
     "start_time": "2025-05-20T13:43:18.735047Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def is_in_tolerance_range(value, exact, tol, mode):\n",
    "    if mode == \"Absolute\":\n",
    "        if torch.abs(exact - value) <= tol:\n",
    "            return True\n",
    "    else:\n",
    "        if torch.abs(exact - value)/value*100.0 <= tol:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def run_sample_paths_experiment(dim, rad, tol, mode, domain, c_vector, num_networks, width, num_samples, max_iter, divisions_per_dimension, c, partition_step, verbose=False):\n",
    "\n",
    "    infeasible = 0\n",
    "    hidden_layers = 1\n",
    "    it_cnt = np.zeros((3, num_networks, num_samples))\n",
    "    failed = [0, 0, 0]\n",
    "    \n",
    "    # for _ in tqdm.tqdm(range(num_networks)):\n",
    "    for _ in range(num_networks):\n",
    "        for j in range(num_samples):\n",
    "            if verbose: print(f\"layers = {layer_sizes}, j = {j}\")\n",
    "            layer_sizes = [dim] + [width] * hidden_layers + [1]\n",
    "            test_network = get_trained_network(layer_sizes, rad=rad, dim=dim)\n",
    "            simple_prob = LipMIP(test_network, domain, c_vector, verbose=False, num_threads=5)\n",
    "            simple_result = simple_prob.compute_max_lipschitz()\n",
    "            if simple_result is None:\n",
    "                infeasible += 1\n",
    "                continue\n",
    "            LipMIP_result = torch.tensor(simple_result.as_dict()[\"value\"])\n",
    "            \n",
    "            SA = StochasticApproximation(test_network, c_vector, domain)\n",
    "            res = SA.compute(v=False, max_iter=max_iter, exact=LipMIP_result, tol=tol, mode=mode)\n",
    "            it_cnt[0, hidden_layers-1, j] = SA.iteration_count\n",
    "            if not is_in_tolerance_range(res, LipMIP_result, tol, mode):\n",
    "                failed[0] += 1\n",
    "                \n",
    "            SA_eq_div = StochasticApproximationEqDiv(network=test_network, c_vector=c_vector, domain=domain, divisions_per_dimension=divisions_per_dimension)\n",
    "            res = SA_eq_div.compute(v=False, total_iterations=max_iter, exact=LipMIP_result, tol=tol, mode=mode)\n",
    "            it_cnt[1, hidden_layers-1, j] = SA_eq_div.iteration_count\n",
    "            if not is_in_tolerance_range(res, LipMIP_result, tol, mode):\n",
    "                failed[1] += 1\n",
    "            \n",
    "            SA_UCB_Dynamic = StochasticApproximationUCBDynamic(test_network, c_vector, domain, c, partition_step)\n",
    "            res = SA_UCB_Dynamic.compute(v=False, max_iter=max_iter, exact=LipMIP_result, tol=tol, mode=mode)\n",
    "            it_cnt[2, hidden_layers-1, j] = SA_UCB_Dynamic.iteration_count\n",
    "            if not is_in_tolerance_range(res, LipMIP_result, tol, mode):\n",
    "                failed[2] += 1\n",
    "        \n",
    "        ### PLOT AFTER EACH LAYER INCREASE >>>>\n",
    "        \n",
    "        plot_results(it_cnt=it_cnt[:, :hidden_layers], dim=DIMENSION, tol=tol, mode=mode, num_networks=hidden_layers, width=width, failed=failed, all_number=num_samples * hidden_layers)\n",
    "        \n",
    "        ### <<<<<\n",
    "        \n",
    "        hidden_layers += 1\n",
    "        \n",
    "    print(f\"Infeasible = {infeasible}\")\n",
    "    return it_cnt"
   ],
   "id": "da67eb8c7d2ad5d8",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T13:43:18.742721Z",
     "start_time": "2025-05-20T13:43:18.740116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_results(it_cnt, dim, tol, mode, num_networks, width, failed, all_number):\n",
    "    num_hidden_layers = list(range(1, num_networks + 1))\n",
    "    if failed[0] != all_number:\n",
    "        plt.errorbar(num_hidden_layers, it_cnt[0].mean(axis=1), yerr=it_cnt[0].std(axis=1), fmt='o-', capsize=5, label=\"Mean Iterations (Original)\")\n",
    "    if failed[1] != all_number:\n",
    "        plt.errorbar(num_hidden_layers, it_cnt[1].mean(axis=1), yerr=it_cnt[1].std(axis=1), fmt='D-', capsize=5, label=\"Mean Iterations (Static partitioning)\")\n",
    "    if failed[2] != all_number:\n",
    "        plt.errorbar(num_hidden_layers, it_cnt[2].mean(axis=1), yerr=it_cnt[2].std(axis=1), fmt='D-', capsize=5, label=\"Mean Iterations (UCB, Dynamic partitioning)\")\n",
    "    plt.xlabel(f'Hidden layers (width = {width})')\n",
    "    plt.ylabel('Sample paths')\n",
    "    # plt.yscale('log')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.grid()\n",
    "    plt.title(f'Number of sample paths for approximation with {mode} Tolerance = {tol}{\"%\" if mode == \"Relative\" else \"\"}\\n'\n",
    "              f'Input dimension = {dim}\\n'\n",
    "              f'Sample paths limit exceeded (No partitioning): {failed[0]}/{all_number} ({failed[0]/all_number*100:.2f}%)\\n'\n",
    "              f'Limit exceeded (Static partitioning): {failed[1]}/{all_number} ({failed[1]/all_number*100:.2f}%)\\n'\n",
    "              f'Limit exceeded (UCB, Dynamic partitioning): {failed[2]}/{all_number} ({failed[2]/all_number*100:.2f}%)')\n",
    "    plt.show()"
   ],
   "id": "97b64b6d5d5a2c07",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T13:52:42.303951Z",
     "start_time": "2025-05-20T13:44:59.746843Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DIMENSION = 7\n",
    "radius = 2.0\n",
    "domain = Hyperbox.build_custom_hypercube(DIMENSION, center=0.0, radius=radius)\n",
    "c_vector = torch.tensor([1.0])\n",
    "\n",
    "tol = 5.0\n",
    "num_networks = 4\n",
    "width = 8\n",
    "num_samples = 30\n",
    "max_iter = 20000\n",
    "mode = \"Relative\"\n",
    "it_count = run_sample_paths_experiment(dim=DIMENSION, tol=tol, mode=mode, domain=domain, c_vector=c_vector, num_networks=num_networks, width=width, num_samples=num_samples, max_iter=max_iter, verbose=False, divisions_per_dimension=2, c=10, partition_step=2, rad=radius)"
   ],
   "id": "abb94c7994531bdf",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "failed = [0, 0, 0]\n",
    "failed[0] = 104\n",
    "failed[1] = 108\n",
    "failed[2] = 85\n",
    "plot_results(it_cnt=it_count, dim=DIMENSION, tol=tol, mode=mode, num_networks=num_networks, width=width, failed=failed, all_number=num_samples * num_networks)"
   ],
   "id": "526a03aa61196694",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a9e8c380ff6dc11e",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
